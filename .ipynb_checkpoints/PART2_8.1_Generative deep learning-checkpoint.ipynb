{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVw0N9jrkIo_"
   },
   "source": [
    "# Chapter8 : Generative deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cppGvZWTkIpA"
   },
   "source": [
    "In this chapter, we'll explore from various angles the potential of deep learning to augment artistic creation. We'll review sequence data generation (which can be used to generate text or music), DeepDream, and image generation using both variational autoencoders and generative adversarial networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qSFA74ckIpB"
   },
   "source": [
    "## 8.1 Text generation with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ku1hgcykIpB"
   },
   "source": [
    "In this section, we'll explore how recurrent neural networks can be used to generate sequence data.\n",
    "\n",
    "Sequence data generation is in no way limited to artistic content generation. It has been successfully applied to speech synthesis and to dialogue generation for chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1z5fQIxkIpC"
   },
   "source": [
    "### 8.1.1 A brief history of generative recurrent networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gxr9MImlkIpC"
   },
   "source": [
    "### 8.1.2 How do you generate sequence data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZOlR8ezFkIpD"
   },
   "source": [
    "The universal way to generate sequence data in deep learning is to train a network to predict the next token or next few tokens in a sequence, using the previous tokens as input. *Tokens* are typically words or characters, and any network that can model the probability of the next token given the previous ones is called a *language model*. A language model captures the *latent space* of language: its statistical structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_7m1V4SkIpD"
   },
   "source": [
    "Once you have such a trained model, you can *sample* from it: you feed it an initial string of text, ask it to generate the next character of next word, add the generated output back to the input data, and repeat the process many times. This loop allows you to generate sequences of arbitrary length that reflect the structure of the data on which the model was trained: sequences that look *almost* like human-written sentences.\n",
    "\n",
    "In the example we present in this section, you'll take a LSTM layer, feed it strings of *N* characters extracted from a text corpus, and train it to predict character *N+1*. The output of the model will be a softmax over all possible characters: a probability distribution for the next character. This LSTM is called a *character-level neural language model*.\n",
    "\n",
    "<img src='image/fig81.PNG' width='550'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3se4szWkIpE"
   },
   "source": [
    "### 8.1.3 The importance of the sampling strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOec_i4HkIpE"
   },
   "source": [
    "When generating text, the way you choose the next character is crucially important. A naive approach is *greedy sampling*, consisting of always choosing the most likely next character. A more interesting approach makes slightly more surprising chocies: it introduces randomness in the sampling process, by sampling from the probability distribution for the next character. This is called ***stochastic sampling***.\n",
    "\n",
    "Sampling probabilistically from the softmax output of the model is neat: it allows even unlikely characters to be sampled some of the time, generating more interesting looking sentences and sometimes showing creativity by coming up with new, realistci sounding words that didn't occur in the training data. But there's one issue with this strategy: it doesn't offer a way to *control the amount of randomness* in the sampling process.\n",
    "\n",
    "When sampling from generative models, it's always good to explore different amounts of randomness in the generation process. Less entropy will give the generated sequences a more predictable structure, whereas more entropy will result in more surprising and creative sequences.\n",
    "\n",
    "In order to control the amount of stochasticity in the sampling process, we'll introduce a parameter called the *softmax temperature* that characterizes the entropy of the probability distribution used for sampling: it characterizes how surprising or predictable the choice of the next character will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjB52AISkIpF"
   },
   "source": [
    "#### Reweighting a probability distribution to a different temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhuvydZBkIpF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# `original_distribution` is a 1D Numpy array of probability values\n",
    "# that must sum to 1.\n",
    "# `temperature` is a factor quantifying the entropy of the ouput distribution.\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    # Returns a reweighted version of the original distribution.\n",
    "    return distribution / np.sum(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvvXMPL0kIpK"
   },
   "source": [
    "<img src='image/fig82.PNG' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQXYbEuVkIpK"
   },
   "source": [
    "> Higher temperatures result in sampling distributions of higher entropy that will generate more surprising and unstructured generated data, whereas a lower temperature will result in less randomness and much more predictable generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL1ysV2zkIpK"
   },
   "source": [
    "### 8.1.4 Implementing character-level LSTM text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agqDAfQ4kIpL"
   },
   "source": [
    "Let's put these ideas into practice in a Keras implementation. In this example, you'll use some of the writings of Nietzsche, the late-nineteenth century German philosopher. The language model you'll learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather a more generic model of the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VYEwq1xkIpL"
   },
   "source": [
    "#### Downloading and parsing the initial text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "NUquiskSkIpM",
    "outputId": "0a209285-19b8-49d6-8386-e2cd4cb4fbf5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 1s 1us/step\n",
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "        'nietzsche.txt',\n",
    "        origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqC5oNCXkIpR"
   },
   "source": [
    "#### Vectorizing sequences of characters\n",
    "\n",
    "Next, you'll extract partially overlapping sequences of length `maxlen`, one-hot encode them, and pack them in a 3D Numpy array x of shape *(shape, maxlen, unique_characters)*. Simultaneously, you'll prepare an array *y* containing the corresponding targets: the one-hot-encoded characters that come after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "GZcN77RokIpR",
    "outputId": "d9e793fb-8e34-4c1a-c943-68d1f28f8a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique charcters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# extract sequences of 60 characters\n",
    "maxlen = 60\n",
    "\n",
    "# sample a new sequence every three characters\n",
    "step = 3\n",
    "\n",
    "# hold the extracted sequences\n",
    "sentences = []\n",
    "# holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text)-maxlen, step):\n",
    "    sentences.append(text[i:i+maxlen])\n",
    "    next_chars.append(text[i+maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# list of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique charcters:', len(chars))\n",
    "\n",
    "# dictionary that maps unique characters to their index in the list \"chars\"\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# one-hot encodes the characters into binary arrays\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWCNdJTGkIpT"
   },
   "source": [
    "#### BUILDING THE NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJLPlsxvkIpU"
   },
   "source": [
    "This network is a single LSTM layer followed by a Dense classifier and softmax over all possible characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fsYUObLRkIpV"
   },
   "source": [
    "#### Single-layer LSTM model for next-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "RtZN1BIZkIpV",
    "outputId": "8dc0c433-e7aa-4d18-9264-20ac2e987ce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               95232     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 57)                7353      \n",
      "=================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bubWEKATkIpY"
   },
   "source": [
    "#### Model compilation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDsOLuIrkIpZ"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgE3KwAikIpb"
   },
   "source": [
    "#### TRAINING THE LANGUAGE MODEL AND SAMPLING FROM IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDXzlPTfkIpb"
   },
   "source": [
    "Given a trained model and a seed text snippet, you can generate new text by doing the following repeatedly:\n",
    "  1. Draw from the model a probability distribution for the next character, given the generated text available so far.\n",
    "  2. Reweight the distribution to a certain temperature.\n",
    "  3. Sample the next character at random according to the reweighted distribution.\n",
    "  4. Add the new character at the end of the available text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sO-AshZkIpc"
   },
   "source": [
    "#### Function to sample the next character given the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_RpnucgjkIpd"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vB7ZI0AekIpf"
   },
   "source": [
    "#### Text-generation loop\n",
    "\n",
    "Finally, the following loop repeatedly trains and generates text. You begin generating text using a range of different temperatures after every epoch. This allows you to see how the generated text evolves as the model begins to converge, as well as the impact of temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zMuDb0Z6kIpg",
    "outputId": "ab810ec9-aabe-4a96-b7ac-fe4737e4a931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 156s 776us/step - loss: 1.5660\n",
      "\n",
      "\n",
      "--- Generating with seed: he\n",
      "last, as it seems to me, who has offered a sacrifice to h*\n",
      "\n",
      "----- temperature: 0.2\n",
      "he\n",
      "last, as it seems to me, who has offered a sacrifice to her a compers of the same and the same the same and all the belief the sense and the same and all a compers in the same and all the expressing and all a still and acts and all the compers of the present and the present of the same and the indification of the sense and all the same and all and all the case of the master of the life of the same and the case of the same and in the commen and all and a\n",
      "----- temperature: 0.5\n",
      "ame and the case of the same and in the commen and all and all the man consequently for the same of the exteption and in himself and the same as a values of the present\n",
      "in the day now the bad and man as a sumely and the last to less and nor have been plature of enemys in the preded the from the belie of the same origin of the how has not in the denectation and finally and of be and the really and the last and something is the first, the same as every aster\n",
      "----- temperature: 1.0\n",
      "the last and something is the first, the same as every aster, dodly.\n",
      "\n",
      "dering, a god undesthing up languily impression. it son the perslea,\n",
      "a mist ollings in alfullly\n",
      "af mirh and grenage the dearthely rule as thing how sprin its the valueled prehang makes his mained the are on you haberction and our\n",
      "its in\n",
      "its oflence to be advallers,\n",
      "themplue, to the do-banuces\n",
      "jueted to the new with the really, the morals and in astive feutrary people as a jepury, as suce\n",
      "----- temperature: 1.2\n",
      "he morals and in astive feutrary people as a jepury, as suce overter--itsers rodion with anretable infersoalished men metapty so midh jusitile. theh, aid hapopmens\n",
      "attelf\n",
      "to be a toocruti: when in new merlineshed--weadhnesly fnationful, natorics.\"\n",
      "they illine. unding and how other as itself--but the earonce elpoghesn\n",
      "toy, he sast andndly mincruncler-scon; wiresgratrong volves (labor\n",
      "thour the opprensy, i beno! hitherto whe philosoppiness\n",
      " moming man cultouepoch 2\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 153s 765us/step - loss: 1.5027\n",
      "\n",
      "\n",
      "--- Generating with seed: ted by\n",
      "the arena and the gladiatorial show.\n",
      "\n",
      "\n",
      "142\n",
      "\n",
      "=to sum u*\n",
      "\n",
      "----- temperature: 0.2\n",
      "ted by\n",
      "the arena and the gladiatorial show.\n",
      "\n",
      "\n",
      "142\n",
      "\n",
      "=to sum upon the sense of the still and condition of the present and strumbbent the sense of the sense of the sense and concerning to the sense of the sense of the sense of the general the sense of the sense of the sense and such a sense of the god of the sense that the sense of the sense and super-oring in the conternal of the present the great and sense of the sense of the most case of the still as a mos\n",
      "----- temperature: 0.5\n",
      "nd sense of the sense of the most case of the still as a most will being the fartent of the more possible--which one is a most same a man of the still and expedience of the has he to distinguish to the taste in the spirit, and present trans and than in the criminal expression and the more spirits in the contradicts and not the sense of a subtleng to states and the change of his fagary in the consense and the present men the daffer of the concerning in a su\n",
      "----- temperature: 1.0\n",
      "nse and the present men the daffer of the concerning in a subgle? aid of the mong\n",
      "the lower\n",
      "that learnghth od privaly:, a states that with if one\n",
      "of the profower, shough and ca, and cestatim that because express\n",
      "in allencht view. when\n",
      "the day, have\n",
      "any\n",
      "have beath, to look and\n",
      "steanness in miraling mosul propose to so the goldeng of a meakn and\n",
      "gedulatening evised-ancied; earnce\n",
      "from sig\" is one\n",
      "cloust, as is opposioes every goetity; one a nothing methous r\n",
      "----- temperature: 1.2\n",
      "oust, as is opposioes every goetity; one a nothing methous repay. that his puines fron; and\n",
      "prejudice of tils: i if i have no comainxmosal\n",
      "men happenisp\":--ut emycept\"-de a up brungual perpois. has haverers\n",
      "of hist thought; amplaody, has vential leines--whereeveation: thetonesned, forously sursines\n",
      "a.=and ansthrery--nature and myteduate,\n",
      "enosy oben to\n",
      "sfule\n",
      "enty what werewarest--he souls ve that eminic.\n",
      "incediant\n",
      "the before, mepeppet-gre;\n",
      "sty e loses moch=epoch 3\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 153s 765us/step - loss: 1.4666\n",
      "\n",
      "\n",
      "--- Generating with seed: s and counter to good taste,--which is always a foretaste of*\n",
      "\n",
      "----- temperature: 0.2\n",
      "s and counter to good taste,--which is always a foretaste of the exages and and sensiment of the sensiment of the sensiment of the more and some man in the same the sensiment of the same the same some prefernally and the profound and something and some the are the superiority of the same be some more and some some such a man in the soul and seems to the same and some the such a metaphysical and such a man of the same powerful of the sure of the property, a\n",
      "----- temperature: 0.5\n",
      "ch a man of the same powerful of the sure of the property, and\n",
      "one yet the same and spirit, and divinion to sicklas and the superiority and serious to society when the long are to see should can the man first and pression of all instance and using to dingind to men to who are in the enlighten man to make the terring singlence of the man will and like all himself and being of everything in the determinal far\n",
      "the contempt and element in the restructions in s\n",
      "----- temperature: 1.0\n",
      "rminal far\n",
      "the contempt and element in the restructions in some quiretement\n",
      "internally everltion. what man, and pelerally out\n",
      "from life-dreames, out of the perfect\n",
      "in such accertances as let to be unity,\n",
      "religion know\n",
      "in the low, itself it is anythologingishurs grustonds, mach its espirally only phenolic others of contemnt of dveryperior\" centum. \"not femuntry, arifowarx and schopenhauer--from as always, it will one, then-who rafered, and emosing ourselves\n",
      "----- temperature: 1.2\n",
      "always, it will one, then-who rafered, and emosing ourselves ?to to genstivelay resiremy opsilation (art will to gade good\n",
      "himself woure borism. not under all himks of the sbquiouimaor by \"confound fundantackesly tort has frody of the undene)ative dogremers: and\n",
      "selmodinged anody man some the lightloy apperation spory easily left of conjuct by cortifes feeling\n",
      "reliwat, life beloid the sangeal to plemocasite view. he one point, is acts on the highers than oepoch 4\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 154s 769us/step - loss: 1.4403\n",
      "\n",
      "\n",
      "--- Generating with seed:  sense (or the capacity for divining quickly\n",
      "the order of ra*\n",
      "\n",
      "----- temperature: 0.2\n",
      " sense (or the capacity for divining quickly\n",
      "the order of race and sense of the same interpretation of the processical and the sense of the sentiment of the sense of the same the procession of the powerful and the same and the processically and the processing of the processical sense of the preciar of the person and the processical still and the procian to the sense of the sense of the same one of the procian to the procianing and sense of the processical \n",
      "----- temperature: 0.5\n",
      " the procian to the procianing and sense of the processical higher propasion of the fore is the persisted, a to the germany revenge, and the presences, which of the procianing to the self-say the powerful, the sense of all the self-alth to its philosopher of the philosopher have to processes, and plact, as all the\n",
      "indispendent are resuraly the\n",
      "self-self, the distinguish to the heart and the self-dessions and instincts of the taste in the touster, this prom\n",
      "----- temperature: 1.0\n",
      "essions and instincts of the taste in the touster, this promi\"ing\n",
      "and personic feducts of thereary becomes resuct, it must have seek so thoughths of say bay agasted.\n",
      "\n",
      "pals of ro mest, orunes! \n",
      "or not have been\n",
      "onvant nature pain, of sb? his far amongicate mray all,\n",
      "proudd such as\n",
      "procaling, conduct to a father\" in the modrance, like; the tust, it is\n",
      "perpation. a far usiforticalic, seeking \"in\n",
      "this words,-is raclpy one of\n",
      "\"falled\n",
      "rright to wherestriticall c\n",
      "----- temperature: 1.2\n",
      " words,-is raclpy one of\n",
      "\"falled\n",
      "rright to wherestriticall carent masternzs\n",
      "iveterntly and dotmoxicating--they\n",
      "habut torakener have beenerable.\n",
      "\n",
      "knoen, aud emotion.--but this propretiticalities entural, when en.\" cades h 1gians!--. have\n",
      "not to the hgirst,\", is me\n",
      "timl illogicion noithnes. bivisnom the incistolsy,\" wfut race\n",
      "have\n",
      "religion\n",
      "and all to lecluce as he possible:\n",
      "who philosophere\"\n",
      "son aways--which! must consentualiny. whyteverse, could melility.\n",
      "\n",
      "epoch 5\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 154s 770us/step - loss: 1.4220\n",
      "\n",
      "\n",
      "--- Generating with seed: tative and restorative power which is the very sign of\n",
      "vigor*\n",
      "\n",
      "----- temperature: 0.2\n",
      "tative and restorative power which is the very sign of\n",
      "vigorous in the profound the world to the more of the problem to the problem of the religion of the religion of the problem to the problem of the distrust the more some the same thing of the problem of the strength of the conscience of the strength of the problem of the strength of the conscience of the stronger of the strength and the great to destent of the promised to the strength of the more thing \n",
      "----- temperature: 0.5\n",
      "o destent of the promised to the strength of the more thing of all them of the world of him which and not more incumis the restines the voical and former than the comminity, thought, what we has dominations of the great follow of the more from all one to have to moral in the interest that the one of the great point of the democration to self-disconscience estimates to have been to the spirit the domanic of the eyes all the things of person foolds and prohe\n",
      "----- temperature: 1.0\n",
      "omanic of the eyes all the things of person foolds and proheced, tradition, civilistical actions had act of thus, its prohest that stomawinith of some higher ac cause one pariorse pleasure, takely contrary. it is happensable to \"conduced in its reninal of conviction, there is gives some to willing teemed to thind when the first, evil, agrance and\n",
      "tertuuets, neetage \"home, from those has heil such potenily question of moralitical self-swank have distrustlic\n",
      "----- temperature: 1.2\n",
      "potenily question of moralitical self-swank have distrustliced as homety, immet or aod of the presead--the !durdibies, their will \"obarvious responsibili re, its sy mune?and moreaqubitious iny if are does the syfter -this happice\n",
      "hus ops which wus and\n",
      "wlunks. he whom and neds;\n",
      "shounce, and good divingues to\n",
      "why there is to lick: that -futficities of the sproming too not a\n",
      "procies how of truth. but his has ally such the\n",
      "ut haf all a brought to himselff.\n",
      "sonepoch 6\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 154s 767us/step - loss: 1.4078\n",
      "\n",
      "\n",
      "--- Generating with seed: er is to be compared.\n",
      "he sees at a glance all that could sti*\n",
      "\n",
      "----- temperature: 0.2\n",
      "er is to be compared.\n",
      "he sees at a glance all that could still and all the problem of the conscience of the strength of the strength of the moral problem of the sense of the morality of the discoverory, and a more of the distrustful and stronger and some that the same and the problem of the strength of the condition of the problem of the factless to the strength and so thing of the strength of the strength of the strength of the strength of the conscience \n",
      "----- temperature: 0.5\n",
      " strength of the strength of the strength of the conscience of the sense of the more and sense and so man as men and man strange, some the philosophers of the conscience and respect and the and the harmy of the same for the dangerous the distinguish to the restrusticism, the soul, that is the more out of the more problem of crimic distinguished, that the strength of the harmy to clears a man of the infficed and every advance of the strongers of the problem\n",
      "----- temperature: 1.0\n",
      "e infficed and every advance of the strongers of the problem, only than thus this regarded, the givel teach noveral recognists no the german an a deeping to old general other trouble good wakes obedience: and for what culteress must, and\n",
      "would uncertdities wanks\n",
      "and sausted, still perhaps himself.\n",
      "pleasmance in germans and disguised, the science effect, mand, may less the problem of the worsp of here\n",
      "what sly attameet hereter slais differers of everythinge\n",
      "----- temperature: 1.2\n",
      "ere\n",
      "what sly attameet hereter slais differers of everythingeroble striedogre exymen: and\n",
      "afte, aestaysublicks, nate like tangneslty, the callounce.\"\"--before must is thrath\" the personal snever labor, assugt for childistianty alestss), that\n",
      "a masisustes, norxitia shurbron all of all to smalily\n",
      "pare,  which have hbrough new hand them losk eventures admitalsal-tream--in sguch would enficuls to creaking. each distrist, ebsud\n",
      "theredy, whiese, itse, some which epoch 7\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 155s 773us/step - loss: 1.3950\n",
      "\n",
      "\n",
      "--- Generating with seed: the rise and fall of the\n",
      "balance of cheerfulness and despair*\n",
      "\n",
      "----- temperature: 0.2\n",
      "the rise and fall of the\n",
      "balance of cheerfulness and despair of the contemporary and self-contemporary the structure of the stronger that the seriousness of the structure of the same the same in the conscience of the sense of the strong and the conscience and the conscience of the decidion of the stronger of the contemporary and the same the conscience of his has the soul, and and the stronger of the problem of the sense of the stronger of the problem of t\n",
      "----- temperature: 0.5\n",
      "the problem of the sense of the stronger of the problem of the pression and the men who in the expression of the greatness of the conscience of the present decidion of the highest the stoolow is themselves the sense the strange of\n",
      "his own badication of the strong and the master\n",
      "the problem of the reconmes in the most philosophy, and that he we had been long that acts and stronger, and the man and stranges one of such a contempt that the same forget and cas\n",
      "----- temperature: 1.0\n",
      "stranges one of such a contempt that the same forget and castle, and to which which a ratical \"naded than causes thereby\n",
      "acklow.\n",
      "\n",
      "1eut ea it the\n",
      "slading himself have those ladient. in the bority of the grodnes of good attapancuatesure in the exceptional tol presumption of good young the enaver a type of out of free thughter churgnishry on the laughed for instance considerngially error that, you how may in pleasm \"comporence gro read from character which on\n",
      "----- temperature: 1.2\n",
      "w may in pleasm \"comporence gro read from character which on\n",
      "that personality.\n",
      "\n",
      "\n",
      " \n",
      "th thicald--which he who in deatter\n",
      "not isly his far 7raves seviests\"\", ttrare of\n",
      "marrow,\n",
      "who impation and\n",
      "son odand of may relinist orses\"--it cannecevear eff \"leaves coutder.\n",
      "\n",
      "cart of future. evirening and the dender of thoefe\" blood: these rolo with occulused. he has wihe\n",
      "that the\n",
      "groft hauds, fromouharfjusy-sufe an\n",
      "atolyond, and a\n",
      "things truth that\n",
      "anotherness nor fortd,epoch 8\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 152s 759us/step - loss: 1.3835\n",
      "\n",
      "\n",
      "--- Generating with seed: nihilation? i believe the deciding\n",
      "influence, as regards the*\n",
      "\n",
      "----- temperature: 0.2\n",
      "nihilation? i believe the deciding\n",
      "influence, as regards the conscience of the philosophers of the promisent the superveption of the superiod and the soul that the philosophers of the promisent the sense of the same the sensual in the sense of the subject of the superioring in the spirit of the superioring of the superilonies of the sense of the supercess and souls of the promisent the subject of the sense of the sense of the most hard in the conscience of\n",
      "----- temperature: 0.5\n",
      "the sense of the sense of the most hard in the conscience of the present the present conception of the present the short of conscience of the conscience with us that with the world of man of an artiful considering that it is this perhaps that the will in the fact as they been constraint the lack of who has the world and former honor, the sensual and religion, and a same to generally to mankind of the yess in the demands--he has a possible the subject of th\n",
      "----- temperature: 1.0\n",
      "the yess in the demands--he has a possible the subject of the badon-grandce, evil sensed, hesis in the utposries, a self leakely, adoragin hy, has arrive heredity.\"\n",
      "thrould we \"clough clomon there-peevert amongene,\n",
      "with ybeingn and\n",
      "with look of our timesm deyes go workd longerop in us who hol, through the rarkous elsisposisf in general being i not cave ha respect eyes was supeny manifest he of humanity, so plenations, and dalian, lugious men ard. \"jest ide\n",
      "----- temperature: 1.2\n",
      "anity, so plenations, and dalian, lugious men ard. \"jest ideals because--som nims--man of it.\n",
      "\n",
      "gof kitfers our ind,onequally conservedy slobred that cears\n",
      "of venerilenis; to\n",
      "mvecolation and disequally betray, sthorler, against emose systemplious.\n",
      "\n",
      "no\n",
      "caussny of \"with a general oef feel you\n",
      "faito. and to emotred..\"--we deed tyancely:--nanpoftige, that foundation, with ho\n",
      "case know this helozed with it, we body, man to ? \n",
      " a chooling with good misus back itsepoch 9\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 152s 760us/step - loss: 1.3747\n",
      "\n",
      "\n",
      "--- Generating with seed: ealing, beneficial and grateful as those delusions! but ther*\n",
      "\n",
      "----- temperature: 0.2\n",
      "ealing, beneficial and grateful as those delusions! but there is a soul to the same to be a seems to be as a sould to the sense of the sense of the self-consequence of the self-consirest to the self-desire to the sense of the subsers and the self-limitance of his faculty of the serve the self-consequence of the self-consequence of the same to the sense of the serve the self-consequence and with the self--and the german of the sense of the self-consequence \n",
      "----- temperature: 0.5\n",
      "e self--and the german of the sense of the self-consequence to pleness of the fact that the perhaps the subsersorial and wholly be opposite most self-religious even as sometimes to the sense of the learned and possibleness, and and interpretation of the serve the profound, which is a consequence and every deepter who around about the face to an acts which the sentiment. in the last been a man, in the later and seems to be later and doubt\n",
      "of the most person\n",
      "----- temperature: 1.0\n",
      "the later and seems to be later and doubt\n",
      "of the most person must a surmoindly\n",
      "privily nature in will\n",
      "fully addital, in lise, with the hardeds, or, and of faith that immo, of many conscious of significancean germans of happerenticled in the way\n",
      "affords, the\n",
      "always been longing of the creatily useful. that the pnoected, disposed to be astathy sthey for well alveept, ripherathed\n",
      "ninkorl. their moral feels according to bring of\n",
      "intelliwnal,\" is to be only be \n",
      "----- temperature: 1.2\n",
      " feels according to bring of\n",
      "intelliwnal,\" is to be only be infemence himself to him and by idealisty, one \"yet asuar\n",
      "thing, under times,\n",
      "onever can rimaver may simple's\n",
      "a romangy: betheousness, inflictoration,, whe was s fancul being famous ctring,\n",
      "or, hefic effacise seems relentive, but ple: the opinion nature of throm which hs so uneti-vationily\n",
      "can\n",
      "to, doe now past to though snotic conditional ptome also omperned\n",
      "amand usrs to rear \"fode? but\n",
      "and loned"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "# trains the model for 60 epochs\n",
    "for epoch in range(1, 10):\n",
    "    print('epoch', epoch)\n",
    "    # fits the model for one iteration on the data\n",
    "    model.fit(x, y, batch_size=128, epochs=1)\n",
    "    # selects a text seed at random\n",
    "    start_index = random.randint(0, len(text)-maxlen-1)\n",
    "    generated_text = text[start_index:start_index+maxlen]\n",
    "    print('\\n\\n--- Generating with seed: ' + generated_text + '*')\n",
    "    \n",
    "    # tries a range of different sampling temperatures\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('\\n----- temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "        \n",
    "        # generates 400 characters, starting from the seed text\n",
    "        for i in range(400):\n",
    "            # one-hot encodes the characters generated so far\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "            # samples the next character\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "            \n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "            \n",
    "            sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDlLcUsnkIpi"
   },
   "source": [
    "> As you can see, a low temperature value results in extremely repetitive and predictable text, but local structure is highly realistic: in particular, all words are real English words. With higher temperatures, the generated text becomes more interesting, surprising, even creative. Also, the local structure starts to break down, and most words look like semi-random strings of characters. Without doubt, 0.5 is the most interesting temperature for text generation in this specific setup.\n",
    "\n",
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that look much more coherent and realistic than this one. But, don't expect to ever generate any meaningful text, other than by random chance: all you're doing is sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there's a distinction between what communications are about and the statistical structure of the messages in which communications are encoded."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PART2_8.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
