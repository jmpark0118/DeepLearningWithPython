{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter5 : Deep learning for computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter introduces convolutional neural networks, also known as ***convnets***, a type of deep learning model almost universally used in computer vision applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Introduction to convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating a small convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28,28,1)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a convnet takes an input tensor of shape (image_height, image_width, image_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a classifier on top of the convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the convnet on MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000,28,28,1))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((10000,28,28,1))\n",
    "test_images = test_images.astype('float32')/255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 43s 717us/step - loss: 0.0354 - acc: 0.9891\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 40s 668us/step - loss: 0.0262 - acc: 0.9917\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 40s 666us/step - loss: 0.0207 - acc: 0.9939\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 39s 644us/step - loss: 0.0164 - acc: 0.9947\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 38s 635us/step - loss: 0.0133 - acc: 0.9958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd44494358>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs = 5,\n",
    "          batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 239us/step\n",
      "test data loss : 0.030063762222915194\n",
      "test data accuracy : 0.9927\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('test data loss :', test_loss)\n",
    "print('test data accuracy :', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Whereas the densely connected network from chapter 2 had a test accuracy of 97.84%, the basic convnet has a test accuracy of 99.27%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this simple convnet work so well, compared to a densly connected model? To answer this, let's dive into what the **Conv2D** and **MaxPooling2D** layers do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 The convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental difference between a densely connected layer and a convolution layer is this:\n",
    "  * **Dense** layers learn ***global patterns*** in their input feature space.\n",
    "  * **Convolution** layers learn ***local patterns***: in the case of images, patterns found in small 2D windows of the inputs.\n",
    "  \n",
    "This characteristic gives convnets two interesting properties:\n",
    "  + *The patterns they learn are translation invariant.*\n",
    "    - After learning a certain pattern in the lower-right corner of a picture, a convnet can recognize it anywhere.\n",
    "    - This makes convnets data efficient when processing images, as they need fewer training samples to learn representations that have generalization power.\n",
    "  + *They can learn spatial hierarchies of patterns.*\n",
    "    - Convnets can efficiently learn increasingly complex and abstract visual concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions are defined by two key parameters:\n",
    "  + *Size of the patches extracted from the inputs*--These are typically 3x3 or 5x5.\n",
    "  + *Depth of the output feature map*--The number of filters computed by the convolution.\n",
    "  \n",
    "Also, note that the output width and height may differ from the input width and height. They may differ for two reasons:\n",
    "  + *Border effects*, which can be countered by padding the input feature map.\n",
    "  + The use of *strides*, which will be defined in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNDERSTANDING BORDER EFFECTS AND PADDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get an output feature map with the same spatial dimensions as the input, you can use *padding*.\n",
    "\n",
    "*Padding* consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit center convolution windows around every input tile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNDERSTANDING CONVOLUTION STRIDES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other factor that can influence output size is the notion of *strides*. The distance between two successive windows is a parameter of the convolution, called its ***stride***, which defaults to 1.\n",
    "\n",
    "*Strided convolutions*, which are convolutions with a stride higher than 1, downsampels the feature map. To downsample feature maps, instead of strides, we tend to use the ***max-pooling*** operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 The max-pooling operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of max pooling is to aggressively downsample feature maps.\n",
    "Max pooling consists of extracting windows from the input feature maps and outputting the max values of each channel.\n",
    "\n",
    "But why downsample feature maps this way? Why not remove the max-pooling layers and keep fairly large feature maps all the way up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 22, 22, 64)        36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_no_max_pool = models.Sequential()\n",
    "\n",
    "model_no_max_pool.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28,28,1)))\n",
    "model_no_max_pool.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "model_no_max_pool.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "\n",
    "model_no_max_pool.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Two things are wrong with this setup\n",
    ">  + It isn't conducive to learning a spatial hierarchy of features.\n",
    ">  + The final feature map has too many parameters. This is far too large for such a small model and would result in intense overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the reason to use downsampling is **to reduce the number of feature-map coefficients to process**, as well as to **induce spatial-filter hierarchies** by making successive convolution layers look at increasingly large windows.\n",
    "\n",
    "Note that max pooling isn't the only way you can achieve downsampling. However, features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map, and it's more informative to look at the *maximal presence* of different features then at their *average presence*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
