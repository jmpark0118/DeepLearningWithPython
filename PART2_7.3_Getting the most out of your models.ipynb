{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "PART2_7.3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LqGgXOPozrh",
        "colab_type": "text"
      },
      "source": [
        "# Chapter7: Advanced deep-learning best practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMm33mZNozri",
        "colab_type": "text"
      },
      "source": [
        "## 7.3 Getting the most out of your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp24lRKkozrj",
        "colab_type": "text"
      },
      "source": [
        "In this section, we'll go beyond \"works okay\" to \"works great and wins machine-learning competitions\" by offering you a quick guide to a set of must-know techniques for building state-of-the-art deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFlnyZRRozrk",
        "colab_type": "text"
      },
      "source": [
        "### 7.3.1 Advanced architecture patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxDHCL4sozrl",
        "colab_type": "text"
      },
      "source": [
        "#### BATCH NORMALIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpr7-Kx_ozrl",
        "colab_type": "text"
      },
      "source": [
        "*Normalization* is a broad category of methods that seek to make different samples seen by a machine-learning model more similar to each other, which helps the model learn and generalize well to new data.\n",
        "\n",
        "Previous examples normalized data before feeding it into models. But data normalization should be a concern after every transformation operated by the network: even if the data entering a `Dense` or `Conv2D` network has a 0 mean and unit variance, there's no reason the expect a priori that this will be the case for the data coming out.\n",
        "\n",
        "Batch normalization is a type of layer that can adaptively normalize data even as the mean and variance change over time during training. The main effect of batch normalization is that it helps with gradient propagation and thus allow for deeper networks. Some very deep networks can only be trained if they include multiple `BatchNormalization` layers.\n",
        "\n",
        "The `BatchNormalization` layer is typically used after a convolutional or densely connected layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvgrcNo1ozrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After a Conv layer\n",
        "conv_model.add(layers.Conv2D(32, 3, activation='relu'))\n",
        "conv_model.add(layers.BatchNormalization())\n",
        "\n",
        "# After a Dense layer\n",
        "conv_model.add(layers.Dense(32, activation='relu'))\n",
        "conv_model.add(layers.BatchNormalization())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccBS7ilZozrq",
        "colab_type": "text"
      },
      "source": [
        "#### DEPTHWISE SEPARABLE CONVOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxOu2JWsozrr",
        "colab_type": "text"
      },
      "source": [
        "*Depthwise separable convolution* layer performs a spatial convolution on each channel of its input, independently, before mixing output channels via a pointwise convolution. This is **equivalent to separating the learning of spatial features and the learning of channel-wise features**, which makes a lot of sense if you assume that spatial locations in the input are highly correlated, but different channels are fairly independent.\n",
        "\n",
        "Depthwise separable convolution requires fewer parameters and involves fewer computations, thus resulting in smaller, speedier models. And because it's a more representationally efficient way to perform convolution, it tends to learn better representations using less data, resulting in better-performing models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhRCva0-ozrr",
        "colab_type": "text"
      },
      "source": [
        "<img src='image/fig716.PNG' width='550'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72qrPsV7ozrs",
        "colab_type": "text"
      },
      "source": [
        "These advantages become especially important when you're training small models from scratch on limited data.\n",
        "\n",
        "Here's how you can build a lightweight, depthwise separable convnet for an image-classification task (softmax categorical classificiation) on a small dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZYk9MSlozrt",
        "colab_type": "text"
      },
      "source": [
        "##### Training a Depthwise separable convolution on the CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5u--BBwozrt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "ac590016-fa58-40b2-c06c-443735e65536"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# input image dimensions\n",
        "height = 32\n",
        "width = 32\n",
        "channels = 3\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], height, width, channels)\n",
        "x_test = x_test.reshape(x_test.shape[0], height, width, channels)\n",
        "input_shape = (height, width, channels)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 10s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tkduXopozrx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "78afd10a-3e00-4e99-d48a-ba2831742ff3"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 12\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.SeparableConv2D(32, 3,\n",
        "                                 activation='relu',\n",
        "                                 input_shape=(height, width, channels,))) \n",
        "model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2))\n",
        "model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n",
        "model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2)) \n",
        "model.add(layers.SeparableConv2D(64, 3, activation='relu')) \n",
        "model.add(layers.SeparableConv2D(128, 3, activation='relu')) \n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "separable_conv2d_7 (Separabl (None, 30, 30, 32)        155       \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 28, 28, 64)        2400      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 12, 12, 64)        4736      \n",
            "_________________________________________________________________\n",
            "separable_conv2d_10 (Separab (None, 10, 10, 128)       8896      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_11 (Separab (None, 3, 3, 64)          9408      \n",
            "_________________________________________________________________\n",
            "separable_conv2d_12 (Separab (None, 1, 1, 128)         8896      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 38,949\n",
            "Trainable params: 38,949\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9N73-Btozr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "bb3268fc-b78c-47d6-cb31-8fc54005d8d0"
      },
      "source": [
        "from keras import optimizers\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2,\n",
        "          verbose=2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            " - 10s - loss: 2.3028 - acc: 0.0969 - val_loss: 2.3027 - val_acc: 0.0980\n",
            "Epoch 2/12\n",
            " - 10s - loss: 2.3027 - acc: 0.0993 - val_loss: 2.3028 - val_acc: 0.0977\n",
            "Epoch 3/12\n",
            " - 10s - loss: 2.3027 - acc: 0.1002 - val_loss: 2.3028 - val_acc: 0.0952\n",
            "Epoch 4/12\n",
            " - 10s - loss: 2.3027 - acc: 0.1002 - val_loss: 2.3027 - val_acc: 0.0952\n",
            "Epoch 5/12\n",
            " - 10s - loss: 2.3027 - acc: 0.1009 - val_loss: 2.3027 - val_acc: 0.0952\n",
            "Epoch 6/12\n",
            " - 10s - loss: 2.3027 - acc: 0.0999 - val_loss: 2.3027 - val_acc: 0.0952\n",
            "Epoch 7/12\n",
            " - 10s - loss: 2.3027 - acc: 0.0972 - val_loss: 2.3027 - val_acc: 0.0952\n",
            "Epoch 8/12\n",
            " - 10s - loss: 2.3027 - acc: 0.0989 - val_loss: 2.3027 - val_acc: 0.0997\n",
            "Epoch 9/12\n",
            " - 10s - loss: 2.3027 - acc: 0.0999 - val_loss: 2.3027 - val_acc: 0.0952\n",
            "Epoch 10/12\n",
            " - 10s - loss: 2.3027 - acc: 0.1000 - val_loss: 2.3027 - val_acc: 0.0952\n",
            "Epoch 11/12\n",
            " - 10s - loss: 2.3027 - acc: 0.0991 - val_loss: 2.3028 - val_acc: 0.0952\n",
            "Epoch 12/12\n",
            " - 10s - loss: 2.3027 - acc: 0.0992 - val_loss: 2.3027 - val_acc: 0.0952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7feb3fd6c240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjTvB12Jozr2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "76ca7b71-d1c5-4b85-9bf6-70dd0caca183"
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 2.302615216827393\n",
            "Test accuracy: 0.10000000149011612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlfrfW8tozr4",
        "colab_type": "text"
      },
      "source": [
        "##### Regular training with a simple ConvNet on the CIFAR dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPJVtw3qozr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "d73906ee-4a29-4e7c-9f10-868c2cfdde2d"
      },
      "source": [
        "model_regular = Sequential()\n",
        "model_regular.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                         activation='relu',\n",
        "                         input_shape=input_shape))\n",
        "model_regular.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model_regular.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_regular.add(Dropout(0.25))\n",
        "model_regular.add(Flatten())\n",
        "model_regular.add(Dense(128, activation='relu'))\n",
        "model_regular.add(Dropout(0.5))\n",
        "model_regular.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model_regular.compile(loss='categorical_crossentropy',\n",
        "                      # replace by rmsprop for comparison\n",
        "                      optimizer='rmsprop',\n",
        "                      # Adding accuracy metrics \n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "model_regular.fit(x_train, y_train,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=epochs,\n",
        "                  validation_split=0.2,\n",
        "                  verbose=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            " - 4s - loss: 1.7688 - accuracy: 0.3622 - val_loss: 1.3641 - val_accuracy: 0.5232\n",
            "Epoch 2/12\n",
            " - 3s - loss: 1.3720 - accuracy: 0.5169 - val_loss: 1.2198 - val_accuracy: 0.5667\n",
            "Epoch 3/12\n",
            " - 3s - loss: 1.2191 - accuracy: 0.5694 - val_loss: 1.0976 - val_accuracy: 0.6148\n",
            "Epoch 4/12\n",
            " - 3s - loss: 1.1142 - accuracy: 0.6085 - val_loss: 1.0551 - val_accuracy: 0.6327\n",
            "Epoch 5/12\n",
            " - 3s - loss: 1.0349 - accuracy: 0.6401 - val_loss: 1.0642 - val_accuracy: 0.6330\n",
            "Epoch 6/12\n",
            " - 3s - loss: 0.9707 - accuracy: 0.6620 - val_loss: 0.9883 - val_accuracy: 0.6616\n",
            "Epoch 7/12\n",
            " - 3s - loss: 0.9281 - accuracy: 0.6780 - val_loss: 0.9344 - val_accuracy: 0.6842\n",
            "Epoch 8/12\n",
            " - 3s - loss: 0.8800 - accuracy: 0.6944 - val_loss: 0.9890 - val_accuracy: 0.6709\n",
            "Epoch 9/12\n",
            " - 3s - loss: 0.8395 - accuracy: 0.7095 - val_loss: 1.0395 - val_accuracy: 0.6601\n",
            "Epoch 10/12\n",
            " - 3s - loss: 0.8145 - accuracy: 0.7171 - val_loss: 0.9590 - val_accuracy: 0.6813\n",
            "Epoch 11/12\n",
            " - 3s - loss: 0.7845 - accuracy: 0.7266 - val_loss: 0.9105 - val_accuracy: 0.7001\n",
            "Epoch 12/12\n",
            " - 3s - loss: 0.7534 - accuracy: 0.7384 - val_loss: 0.9828 - val_accuracy: 0.6905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7feb6eee4400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyzbdPZ5ozr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3aec241f-279b-4f2e-d1d7-6bf36aa0ab44"
      },
      "source": [
        "score = model_regular.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.9877223818778992\n",
            "Test accuracy: 0.6887999773025513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsw6_aqozr-",
        "colab_type": "text"
      },
      "source": [
        "> The results are not very convincing. Depthwise separable convolution gives an accuracy of 10% that is inferior to the simple ConvNet model. Probably this is because the model is not big or deep enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xr0w1DEozr-",
        "colab_type": "text"
      },
      "source": [
        "##### Training with Adadelta optimizer on the CIFAR dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqrWbOBTozr_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "243a43f1-5ea1-4b11-9d2e-b40bc85fed1d"
      },
      "source": [
        "model_ada = Sequential()\n",
        "model_ada.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                     activation='relu',\n",
        "                     input_shape=input_shape))\n",
        "model_ada.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model_ada.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_ada.add(Dropout(0.25))\n",
        "model_ada.add(Flatten())\n",
        "model_ada.add(Dense(128, activation='relu'))\n",
        "model_ada.add(Dropout(0.5))\n",
        "model_ada.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model_ada.compile(loss='categorical_crossentropy',\n",
        "                  # The model_ada is optimized with the Adadelta optimizer\n",
        "                  optimizer='Adadelta',\n",
        "                  # Adding accuracy metrics \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "model_ada.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_split=0.2,\n",
        "              verbose=2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            " - 4s - loss: 1.8530 - accuracy: 0.3344 - val_loss: 1.5071 - val_accuracy: 0.4519\n",
            "Epoch 2/12\n",
            " - 3s - loss: 1.4081 - accuracy: 0.5023 - val_loss: 1.3011 - val_accuracy: 0.5500\n",
            "Epoch 3/12\n",
            " - 3s - loss: 1.2078 - accuracy: 0.5731 - val_loss: 1.1021 - val_accuracy: 0.6037\n",
            "Epoch 4/12\n",
            " - 3s - loss: 1.0927 - accuracy: 0.6172 - val_loss: 1.0168 - val_accuracy: 0.6402\n",
            "Epoch 5/12\n",
            " - 3s - loss: 0.9981 - accuracy: 0.6514 - val_loss: 0.9755 - val_accuracy: 0.6581\n",
            "Epoch 6/12\n",
            " - 3s - loss: 0.9269 - accuracy: 0.6776 - val_loss: 0.9298 - val_accuracy: 0.6748\n",
            "Epoch 7/12\n",
            " - 3s - loss: 0.8761 - accuracy: 0.6934 - val_loss: 0.9063 - val_accuracy: 0.6893\n",
            "Epoch 8/12\n",
            " - 3s - loss: 0.8106 - accuracy: 0.7178 - val_loss: 0.9079 - val_accuracy: 0.6817\n",
            "Epoch 9/12\n",
            " - 3s - loss: 0.7635 - accuracy: 0.7351 - val_loss: 0.9855 - val_accuracy: 0.6747\n",
            "Epoch 10/12\n",
            " - 3s - loss: 0.7157 - accuracy: 0.7510 - val_loss: 0.8965 - val_accuracy: 0.6959\n",
            "Epoch 11/12\n",
            " - 3s - loss: 0.6728 - accuracy: 0.7646 - val_loss: 0.8974 - val_accuracy: 0.6976\n",
            "Epoch 12/12\n",
            " - 3s - loss: 0.6321 - accuracy: 0.7788 - val_loss: 0.9001 - val_accuracy: 0.6888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7feb6eda4518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArQn86PTozsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bff86fe6-b097-4927-b404-6e49cf3b444c"
      },
      "source": [
        "score = model_ada.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.9006516153335571\n",
            "Test accuracy: 0.6876999735832214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwQmyMNiozsG",
        "colab_type": "text"
      },
      "source": [
        "### 7.3.2 Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUhp0mHaozsG",
        "colab_type": "text"
      },
      "source": [
        "*Hyperparameters* are architecture-level parameters which are trained via backpropagation. (eg. activation, dropout)\n",
        "\n",
        "The process of optimizing hyperparameters typically looks like this:\n",
        "  1. Choose a set of hyperparameters (automatically).\n",
        "  2. Build the corresponding model.\n",
        "  3. Fit it to your training data, and measure the final performance on the validation data.\n",
        "  4. Choose the next set of hyperparameters to try (automatically).\n",
        "  5. Repeat.\n",
        "  6. Eventually, measure performance on your test data.\n",
        "  \n",
        "Training the weights of a model is relatively easy: you compute a loss function on a mini-batch of data and then use the Backpropagation algorith to move the weights in the right direction. Updating hyperparameters, on the other hand, is extremely challenging. Consider the following:\n",
        "  + Computing the feedback signal can be extremely expensive: it requires creating and training a new model from scratch on your dataset.\n",
        "  + In many cases, you must rely on gradient-free optimization techniques, which naturally are far less efficient then gradient descent.\n",
        "  \n",
        "Overall, hyperparameter optimization is a powerful technique that is an absolute requirement to get to state-of-the-art models on any task or to win machine-learning competitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6CpLWu1ozsH",
        "colab_type": "text"
      },
      "source": [
        "### 7.3.3 Model ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XstUM4GNozsH",
        "colab_type": "text"
      },
      "source": [
        "Another powerful technique for obtaining the best possible results on a task is *model ensembling*. Ensembling consists of pooling together the predictions of a set of different models, to produce better predictions.\n",
        "\n",
        "Ensembling relies on the assumption that different good models trained independently are likely to be good for *different reasons*: each model looks at slightly different aspects of the data to make its predictions, getting part of the \"truth\" but not all of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pItJPeQ1ozsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b78561e7-dddd-4596-9649-7aceffa845e4"
      },
      "source": [
        "x_val = x_test.reshape(10000, 32, 32, 3)\n",
        "\n",
        "# Use three different models to compute initial predictions\n",
        "preds = model.predict(x_val)\n",
        "preds_regular = model_regular.predict(x_val)\n",
        "preds_ada = model_ada.predict(x_val)\n",
        "\n",
        "# This new prediction array should be more accurate than any of the initial ones\n",
        "final_preds = (1/3) * (preds + preds_regular + preds_ada)\n",
        "\n",
        "import numpy as np\n",
        "final_preds_one_hot = np.zeros_like(final_preds)\n",
        "final_preds_one_hot[np.arange(len(final_preds)), final_preds.argmax(1)] = 1\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, final_preds_one_hot)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}